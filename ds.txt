Machine learning is a  subset of AI

in ML we build an engine , module and give it lots of data to learn from
our model will find patterns from our input and learn from it
more data we give , better the accuracy

step 1: import data (generally in a .csv file)

step2: clean data (data should not be duplicated, no irrelevant data ,no incomplete values, convert text data to numerical values)

step3: splitting cleaned data into training  AND  testing
(generally more training data and less testing data %)

step4: creating a model (selecting an algorithm to analyze data eg CNN , decision trees ,  scikit learn)
(choose algo based on problem req)

step 5: train our model (feed him the data , model will look for patterns)

step6: make predictions

step7: evaluate protection , improve accuracy (select diff algo OR fine tune params)

--
popular python libraries for ML 

numpy = multi dimensional array
pandas= data analysis library , provides data frame (its a 2D data structure like xl spreadsheet)
MatPlotLib= 2d ds for creating graphs on plots
sidekick learn = ml lib , provide common algos like decision trees ,neural network

in vscode n other editors it is hard to inspect data 
so we use jupyter
to install jupyter we use anaconda platform

anaconda will install jupyter and libs like numpy n shit


import pandas as pd   //consider pandas as pd
 if we do this we wont have to type pandas . function .fun
  instead pd.func


in jupyter files are saved as .ipynb   (nb= notebook)  # this file has our code in cells AND output of each cell  
thus it has a different extension

```
pd.read_csv("vgsales.csv")  # this returns a dataFrame object
```

```
import pandas as pd
df=pd.read_csv("vgsales.csv")   # load dataset in PANDAS
df                  # NO PRINT NEEDED | this also returns the same
print(df) # same output
```

```
pd.read_csv("vgsales.csv")   # we used relative path cuz csv file is in our folder of helloworld.ipynb



yo1 = pd.read_csv("vgsales.csv")  # this command reads a file IMP PD . read  AND store it in a var "yo1"


yo1.shape  # shows the total records, columns [shows no of rows and columns


yo1.describe() # this method returns the basic statistics . it gives  information of CLOUMNS  / basic stats about our data

yo1.values            # this gives 2D array

```

1 segment = 1 cell

each cell will be executed differently n output will be shown at their respective places

Terminology :

mean = average

standard deviation =  measures amount of variation in values


Shortcuts: 

```
shift + enter          # this runs a cell (if in command(blue) mode it converts to green mode and runs)

d  + d        # this deletes a cell


go to cell->run all   # this runs all cells
```
green = edit mode  [press return/enter for edit]
blue = command mode  (press ESC key to enter this mode)

press h in command mode to see short cuts
b = (below) inserts a new cell at the below
a = (above ) inserts a new cell above
d+d = delete a cell
z = undo
shift + enter =runs the selected cell

yo1.   then press tab  = this shows all the functions in the pandas
press shift + tab = this shows what the command does

press run all  this runs all the cells at once | if not u have to manually select each cell and run it

--


Training a model

whenever we train a model we give input set (X) and output set(y)

output set(y) contains the prediction

based on prev prediction model trains itself n makes new predictions

output set = predictions ( in this expected output form model is given , kinda like sample answers we expect) 


```
X = music_data.drop(columns=["genre"]) # this creates a new dataSet without "genre" column  | THIS DOES NOT EDIT THE ORIGINAL DATA SET
```

don't forget columns =

Algo(we used in this project) = decision tree
decision tree is already programmed in a library called scikit  lib      (its a  library bruh)

The model is the “thing” that is saved after running a machine learning algorithm on training data

```
from sklearn.tree import DecisionTreeClassifier  # sklearn = package , tree = module , DecisionTreeClassifier = class


.fit() # this method takes two data sets , one  input and one output , n we SEND this data for training | TRAINS our model

.predict( [[ ]] )  # this method takes TWO arrays (outer array, inner array) | OR can take direct variable containing dataset
model_data.predict([[21,1], [22,0]])  #  make two predictions on for a 21 yr old male AND one for a 22 year old female   [age,gender]
```

```
2 d array = array inside an array 

```

70% data for training ,  30% data for testing

if you give less training size eg,    train_test_split( X , y , test_size= 0.8) # more space for testing ,less for training
accuracy drops fucking low like 26% accuracy
--
 Finding accuracy of model 


from sklearn.metrics import accuracy_score  // this class contains func related to accuracy


- split data set  into training and  testing  ( train_test_split , this class splits data set )
previously we passed entire data set   eg,                   model.fit(X,y)

-   allocate 70%  TRAINing , 30% testing  (to increase accuracy give more training , if u give less training n more testing Accracy REDUCES)
- instead of giving two samples [20,1][21,0]  give  X_set  to . predictions (this gives us prediction)  (model.fit(X_train, y_train)
- compare data from PREDICTION (X_test) and TEST SET( y_test)  // import accuracy_score , accuracy_score(y_test,predictions)

if u run the program many times  u may see diff in accu , cuz at each runtime a diff random row is selected as test value

from    sklearn.model_selection            import    train_test_split

```
from sk...

train_test_split( X , y , test_size= 0.2)  /* the   keyword argument test_size specifies the size of our data set | this func returns a tuple so we can unpack it into 4 variables  | allocating 20% size for testing  */

X_train , X_test ,  y_train , y_test = train_test_plit( X , y , test_size= 0.2)  # first two sets are for training 2 INPUT (X, here) sets other 2 are testing
# above is  UNPACKING u learnt it before a,b,c=10

model.fit(X_train, y_train)  # this passes only training data (after we split data using train_test_split method)
```



predictions=model_data.predict([[21,1], [22,0]])
instead of the above
predictions=model_data.predict(X_test)  # X_test contains all  testing values

```
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

music_data=pd.read_csv("music.csv")                         # import data
X= music_data.drop(columns=["genre"])     # removing output columns and storing it new dataset in X(input dataset) OR u can select the only column u need
y= music_data["genre"]                    # taking only "genre" column from .csv file storing it in y(output dataset)
y_train ,y_test , X_train, X_train =train_test_split(X,y,test_size=0.2)  # splitting data using train_test.. method

model =  DecisionTreeClassifier()                           # creating an object/model

model.fit(X_train, y_train)                                 # passing specific data

predictions= model.predict(X_test)                          # asking predictions

score=accuracy_score(y_test,predictions)                    # comparing output testing values and Predictions variable
score
```


--
build a model using ML algo

form sklearn.tree import DecisionTreeClassifier       ( this DecisionTreeClassifier  has Decesion tree algo)

model = DecisionTreeClassifier()  // creating a new object named "model' this obj will act as our model (after training algo)

--

Persisting model

previous program cant be used for new model cuz sending data to model n then training it will take hours if data set is big

train our model n save it to a file and reuse it later

from sklearn.externals import joblib  //sometimes this doesn't work so use import joblib directly
joblib is used for saving and loading prexisting models

so after training our model (in the "model" obj ref we made of decision tree classifier class) use 

```
joblib.dump(model , "music_recommender.joblib")   //  stores our model in whatever _name_we _give.joblib
```

```
joblib.load("music_recommender.joblib")    //this opens our .joblib binary file | the file which has our binary
```


make an object "model" (same name as prev obj with passed datasets)

model = joblib.("music_recommender.joblib")

---

Seeing decision trees

from sklearn   import tree                     // exports decision tree in GRAPHICAL format
tree.export(model, out_file="music-recomender.dot", feature_names=["age","gender"])            //  .dot = graph description language | columns are features of our data
                                                                                                                                                                                                 // class names= list of classes like labels like hip-hop ,classical
class_names=sorted(y.unique())
sorted sorts the data in output dataset [y] 
.unique() removes and duplicates eg. jazz doesn't appear twice if we do so

labes=ls="all",rounded =True, filled=True)


every node in binary search tree has labels(roll number ish
we get rounded edged 
colours are filleld




You can capture more factors(price, location etc) using a tree that has more "splits." These are called "deeper" trees. 
The point at the bottom where we make a prediction is called a leaf.The point at the bottom where we make a prediction is called a leaf.

This step of capturing patterns from data is called fitting or training the model. The data used to fit the model is called the training data.

The details of how the model is fit (e.g. how to split up the data)



Pandas is the primary tool data scientists use for exploring and manipulating data.

panda can be used to manipulate  .csv files

dataFrame  has data like tables (kinda ms excel ones)


Kaggel notebook:
has markdown cell: holds only text and its formatting (styling)
code cell: has code that can be ezecuted

To choose variables/columns, we'll need to see a list of all columns in the dataset. That is done with the columns
```
yo1=pd.read_csv("game.csv")
yo1.columns         # prints out names of ALL columns
```


```
import pandas as pd

melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'
melbourne_data = pd.read_csv(melbourne_file_path) 
melbourne_data.columns
# dropna drops missing values in a dataset1 (think of na as "not available")
melbourne_data = melbourne_data.dropna(axis=0)  
```

-- INPUT OUTPUT stuff --

dropna = drop not available data

We'll use the dot notation to select the column we want to predict, which is called the prediction target. By convention, the prediction target is called y
y= output set (prediction set)
```
y = melbourne_data.Price  # selecting ONE column "Price" for prediction from whole fucking dataset
```

--
The columns that are inputted into our model (and later used to make predictions) are called "features."

X= input set (feature set)   // I F 

you will use all columns except the target(output) as features(input)

The steps to building and using a model are:

- Define: What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.
- Fit: Capture patterns from provided data. This is the heart of modeling.
- Predict: used to predict from supplied data
- Evaluate: Determine how accurate the model's predictions are.

Many machine learning models allow some randomness in model training. Specifying a number for random_state ensures you get the same results in each run

```
DecisionTreeRegressor(random_state=1)
```


head method, which shows the top few rows. 
X.head()  

```
print(melbourne_model.predict(X.head()))  //make prediction of head of input datset from melbourne_model
```


predictions = iowa_model.predict(X)
print(predictions) 

Make predictions with the model's predict command using X as the data. Save the results to a variable called predictions.


--

YT course

-data storage n distributed computing : storage and distributed computing => apache hadoop , apache spark
- data visualization : matPlotLib , Jupyter , powerBI , tableau
- deep Learning : TensorFlow ,  pyTorch , sciKitLearn

business problem - > data collection/gathering - > dataCleaning (max time spent in cleaning)-> dataModel(not req for simple problems) -> collect insight  /start predictive analysis


Anaconda  = bunch of python packages and a package manager(to download other packages) called conda
by default we have numpy(DataSci), Scikit learn(ML) ,jupyter ,sciPy

with pip we can install only python dependencies
with conda we can install non-python dependencies

```
conda list        # this lists all the packages intalled
```

jupyter is a webApp that runs live code , used for data visualization , and have explanatory code all in one place

--

data cleaning

types of dirty data
-cleaning data with wrong format
-removing empty data  (NaN values)   and filling empty data
-removing duplicate data

---
removing empty data :

without changing the original df

```
df  = pd.read_csv(path)
new_df= df.dropna()            #new_df is an new version and NOT an edited version |  dropna simply removes the rows with null values (from new df)
print(new_df)
```

with changing the df 

```
df  = pd.read_csv(path)
df= df.dropna(inplace =True)            # making changes to original df  || drops only the missing values(NaN)

df = df.drop ()                         # drops only the specified values
print(df)
```

When using drop only, it drops the columns/rows you define

When using dropna, it removes all entries with NaN values (or null in general)

Filling empty data with given values

```
df.fillna("yolo", inplace = True)       # wherever there is NaN we fill it with "yolo" EVERYWHERE   ,here


df["Total"].fillna("yolo", inplace = True)       # this fills the empty spaces IN "Total" COLUMN ONLY with "yolo"  |not everywhere
```

```
x = df["total_rooms"].mean()          # took the mean
df["mediam_income"].fillna(x,inplace =True)     # select col "median_income" and fillempty values with x (i.e mean) in original dataset
```
  
```
x = df["total_rooms"].mode()           # or .median() if req
df["mediam_income"].fillna(x,inplace =True)
```


---

changing the dataType entered


we can remove the data

or we can edit the dataFrame to a given dataFormat

```
df["Date"] = pd.to_datetime(df["Date"])   # converting the WHOLE column "Date" of  df, using a Pandas method pandas.to_date_time(df["Date"])

                                           # time(df["date"])  this only makes changes in "date" column and not the whole df
```

.loc is used to grab specific rows and columns

```
df.loc[row_name , column_name]      # row_name can be used a sr.no the first unnamed default col in df

df.loc[1 , "amount"]  = 45          # kinda cartesian stuff   sr.no 1 in "amount" column ,change that value to 45

df.loc[1, : ]      # this gives everything present in the 1st row in their respective column (wild card in string mutation yo[:])

df.loc[0:3 , : ]    # this gives first 2 rows and all columns
```


.loc used table values (index)
.loc uses integer values 

```
for x in df.index:
    if df.loc[x,"amount"] >120:
        df.loc[x,"amount"] = 120                    # if the value is greater than 120 make  it 120
```


---
		finding and acting on duplicate values
		
		
```
df.duplicated()              # this gives True if duplicate values are found False if data is unique  |not duplicate but DUPLICATED
df.drop_duplicates(inplac=True)   # this drops the duplicate ROWS
```

